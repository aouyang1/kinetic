/*
See the LICENSE.txt file for this sampleâ€™s licensing information.

Abstract:
The main user interface for the sample app.
*/

import SwiftUI
import AVFoundation
import AVKit

@MainActor
struct CameraView<CameraModel: Camera>: PlatformView {

    @Environment(\.verticalSizeClass) var verticalSizeClass
    @Environment(\.horizontalSizeClass) var horizontalSizeClass

    @State var camera: CameraModel

    var body: some View {
        ZStack {
            // A container view that manages the placement of the preview.
            PreviewContainer(camera: camera) {
                // A view that provides a preview of the captured content.
                CameraPreview(source: camera.previewSource)
                    // Handle capture events from device hardware buttons.
                    .onCameraCaptureEvent(defaultSoundDisabled: true) { event in
                        if event.phase == .ended {
                            let sound: AVCaptureEventSound
                            sound = camera.captureActivity.isRecording ?
                                .endVideoRecording : .beginVideoRecording
                            // Toggle video recording when pressing a hardware button.
                            await camera.toggleRecording()
                            
                            // Play a sound when capturing by clicking an AirPods stem.
                            if event.shouldPlaySound {
                                event.play(sound)
                            }
                        }
                    }
            }
            // The main camera user interface.
            CameraUI(camera: camera)
        }
    }
}

#Preview {
    CameraView(camera: PreviewCameraModel())
}

enum SwipeDirection {
    case left
    case right
    case up
    case down
}
